{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybersecurity Threat ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of our cybersecurity project is to treat and analyse data from a wireshark recording so that we can identify threats. We tried to develop a model which can predict if a record contains a DoS attack.\n",
    "\n",
    "We organized our team with a lead developper who assigned tasks and code to others. There was also an infrastructure leader. All the brainstorms were done together. \n",
    "\n",
    "Technically we divided the project into different files for the data cleaning, the data preparation and the predictions. We also designed our infrastructure with a dockerfile and a docker compose stack. When deployed in production the server runs fastAPI from `app.py`. We calculated some models' metrics that we saved on MlFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Download.py`\n",
    "\n",
    "Firstly we imported kaggle. We defined a function to download the data from kaggle. We printed the path to the dataset during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "def download_dataset() -> str:\n",
    "    # Download latest version and return path of downloaded files\n",
    "    return kagglehub.dataset_download(\"chethuhn/network-intrusion-dataset\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Path to dataset: {download_dataset()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Clean.py`\n",
    "\n",
    "Firstly we did some importations. We ensured that if the user didn't have the libraries he received a message saying that he must install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from download import download_dataset\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print(\"You need to install pandas\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    print(\"You need to install numpy\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a final variable which is the treshold of similarities percentage. The columns with a higher percentage of similarities are dropped.\n",
    "\n",
    "Then we defined a function to calculate the percentage of column similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_SIMILARITY_THRESHOLD = 95\n",
    "\n",
    "def get_percentage_columns_similarity(df):\n",
    "    return (1 - (df.nunique() / len(df))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function allows to load and clean the data. First it concatenates all the csv files in the dataframe \"global_df\". It deletes the spaces thanks to the strip() function. It drops the columns according to the similarity threshold but keeping the labels. It also drops duplicates and all lines whose label is not \"BENIGN\" neither \"DoS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(data_path=download_dataset()):\n",
    "    global_df = pd.DataFrame()\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                global_df = pd.concat([global_df, df], ignore_index=True)\n",
    "\n",
    "    global_df.columns = global_df.columns.str.strip()\n",
    "\n",
    "    # Drop columns according to similarity threshold\n",
    "    columns_to_drop = get_percentage_columns_similarity(global_df)\n",
    "    columns_to_drop = columns_to_drop[columns_to_drop > COLUMN_SIMILARITY_THRESHOLD].index\n",
    "    # Don't drop the label column\n",
    "    columns_to_drop = columns_to_drop.drop('Label')\n",
    "    global_df = global_df.drop(columns=columns_to_drop)\n",
    "    global_df = global_df.replace([float('-inf'), float('inf')], float('nan')).dropna()\n",
    "    # drop duplicates\n",
    "    global_df = global_df.drop_duplicates()\n",
    "    # Drop all lines where the label is not BENIGN or does not contains \"DoS\"\n",
    "    global_df = global_df[global_df['Label'].str.contains('DoS') | global_df['Label'].str.contains('BENIGN')]\n",
    "\n",
    "    return global_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we defined a function to save the features and the labels. It takes as parameters the dataframe cleaned as well as the features and labels paths. First it separates the label colum from the features. It makes the \"BENIGN\" labels become 0 and the others 1. Then it counts the number of labels for each types. Then it extracts the features and the labels from the dataframe. It creates a path for features and a path for labels in case of they don't already exist. It saves the features and the labels in csv files and it prints the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_and_labels(global_df, features_path='data/features.csv', labels_path='data/labels.csv'):\n",
    "    # Separate label column from features\n",
    "    global_df['Label'] = np.where(global_df['Label'].isin(['BENIGN']), 0, 1)\n",
    "    print(\"Labels count : \", global_df['Label'].value_counts())\n",
    "    labels = global_df['Label']\n",
    "    features = global_df.drop(columns=['Label'])\n",
    "\n",
    "    # Create features_path and labels_path's directory if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(features_path)):\n",
    "        os.makedirs(os.path.dirname(features_path))\n",
    "    if not os.path.exists(os.path.dirname(labels_path)):\n",
    "        os.makedirs(os.path.dirname(labels_path))\n",
    "\n",
    "    # Save to csv\n",
    "    features.to_csv(features_path, index=False)\n",
    "    labels.to_csv(labels_path, index=False)\n",
    "\n",
    "    print(f\"Features and labels saved to {features_path} and {labels_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally during the execution we affected to the global_df variable the cleaned dataframe and we saved its features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_df = load_and_clean_data()\n",
    "    save_features_and_labels(global_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Preparation.py`\n",
    "\n",
    "First we did some importations. Pandas allows dataframes manipulation, seaborn and matplotlib allow data visualization, numpy allows math calculations and array manipulation, and RandomUnderSampler allows undersampling.\n",
    "\n",
    "We loaded the features and the labels in dataframes that we returned in the function load_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(features_path, labels_path):\n",
    "    features = pd.read_csv(os.path.abspath(features_path))\n",
    "    labels = pd.read_csv(os.path.abspath(labels_path))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we defined a function whose aim is to do and save a correlation matrix as an image. First we made a copy of the dataframe and we added a column with the labels. Then we calculated the correlation matrix. We plotted it making sure that the lowest values were in blue and the highest in red. We saved the image in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_correlation_matrix(df, labels, filename):\n",
    "    features_with_labels = df.copy()\n",
    "    features_with_labels['Label'] = labels\n",
    "    corr = features_with_labels.corr()\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "    plt.savefig(filename)\n",
    "    print(f\"Correlation matrix saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a function that dropped the most correlated features. For that we defined a threshold. We created the upper triangle of the correlation matrix, we selected the colums with the correlations above threshold and we dropped them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_correlated_features(df, threshold=0.9):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    # Create the upper triangle of the correlation matrix (no need to keep the lower triangle and diagonal)\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # Select columns with correlations above threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data was unbalanced we decided to do some undersampling. We did it in the function resample_data(). We used the RandomUnderSampler() function which undersample the majority class. We applied the undersampling on our data and we saved it into new dataframes that we returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(features, labels):\n",
    "    rus = RandomUnderSampler(random_state=17, sampling_strategy='majority')\n",
    "    features_resampled, labels_resampled = rus.fit_resample(features, labels)\n",
    "    return features_resampled, labels_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a function to save the data into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(features, labels, features_path, labels_path):\n",
    "    features.to_csv(features_path, index=False)\n",
    "    labels.to_csv(labels_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the execution we made some function calls. First we loaded the data, then we calculate and save the correlation matrix multiple times, after loading, after resample and after cleaning. Finally, we saved the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    features, labels = load_data('data/features.csv', 'data/labels.csv')\n",
    "    save_correlation_matrix(features, labels, 'data/corr_matrix.png')\n",
    "    \n",
    "    features_resampled, labels_resampled = resample_data(features, labels)\n",
    "    save_correlation_matrix(features_resampled, labels_resampled, 'data/corr_matrix_resampled.png')\n",
    "\n",
    "    features = clean_correlated_features(features)\n",
    "    save_correlation_matrix(features, labels, 'data/corr_matrix_cleaned.png')\n",
    "    \n",
    "    save_data(features_resampled, labels_resampled, 'data/features_cleaned.csv', 'data/labels_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Prediction.py`\n",
    "\n",
    "In this file we performed some training and testing of different models on data. \n",
    "\n",
    "We used python library sklearn.\n",
    "\n",
    "The models are : \n",
    "- logistic regression\n",
    "- random forest\n",
    "- k-nn\n",
    "- decision tree\n",
    "\n",
    "We also calculated some metrics : \n",
    "- f1 score\n",
    "- precision score\n",
    "- recall score\n",
    "- roc auc score\n",
    "\n",
    "First we defined some global variables for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 5\n",
    "NUM_FOLDS = 4\n",
    "DEBUG = True\n",
    "IS_PROD = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set the uri tracking and the experiment of MlFlow. This allows us to follow the model executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://mlflow.docsystem.xyz\" if IS_PROD else \"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"RandomSearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 2 dataframes from the csv files of features and labels cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(os.path.abspath('data/features_cleaned.csv'))\n",
    "labels = pd.read_csv(os.path.abspath('data/labels_cleaned.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We splitted the data for training (70%) and testing (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the indexes of both train and test data in order to ensure some consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a label column to a copy of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalData = X_test.copy()\n",
    "evalData['label'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a dictionary for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=3000),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also defined a dictionary for the different models' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': np.logspace(-10, 10, 100),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'min_samples_split': np.arange(2, 21),\n",
    "        'max_depth': np.arange(1, 21)\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'n_neighbors': np.arange(1, 21),\n",
    "        'p': np.arange(1, 6)\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': np.arange(1, 21),\n",
    "        'min_samples_split': np.arange(2, 21),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized some variables to store the best model, the best score and the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we made a loop for training and testing each model. We tried to find the best hyperparameters. Then we same the actual model and its parameter distributions in local variables. \n",
    "Then we performed some scaling (except for the random foret model) with StandardScaler() and fit_transform() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    print(\"Tuning hyperparameters for model\", name)\n",
    "    \n",
    "    model = models[name]\n",
    "    param_dist = param_distributions[name]\n",
    "    \n",
    "    if name != \"Random Forest\":\n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still in the loop, we defined a function calculating the f1 score of real labels and predictions made by the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scoringfunc(estimator, X, y):\n",
    "        return f1_score(y, estimator.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the RandomizedSeachCV() function to find the best hyperparameters. Then we trained the model with those best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=NUM_TRIALS,\n",
    "        scoring=scoringfunc,\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    randomized_search.fit(X_train_scaled, y_train.values.ravel())\n",
    "    \n",
    "    best = randomized_search.best_params_\n",
    "    model.set_params(**best)\n",
    "    model.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculated the f1 score and obtained the signature of the model. Then we use this signature in the MlFlow recording and evaluation. MlFlow calculates the metrics and finds the best model based on the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    f1_test = f1_score(y_test, model.predict(X_test_scaled))\n",
    "    \n",
    "    signature = infer_signature(X_train_scaled, model.predict(X_train_scaled))\n",
    "    \n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "        mlflow.log_params(best)\n",
    "        mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "        mlflow.log_metric(\"F1_Score\", f1_test)\n",
    "        mlflow.log_metric(\"Accuracy\", model.score(X_test_scaled, y_test))\n",
    "        mlflow.log_metric(\"Precision\", precision_score(y_test, model.predict(X_test_scaled)))\n",
    "        mlflow.log_metric(\"Recall\", recall_score(y_test, model.predict(X_test_scaled)))\n",
    "        mlflow.log_metric(\"AUC\", roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1]))\n",
    "        \n",
    "        if f1_test > best_score:\n",
    "            best_score = f1_test\n",
    "            best_model = model\n",
    "            best_params = best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we saved the best parameters and scores in a MlFlow experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"BestModelParams\")\n",
    "name = best_model.__class__.__name__\n",
    "with mlflow.start_run(run_name=name) as run:\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"f1\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `app.py`\n",
    "\n",
    "We constructed an API thanks to FastAPI from fastapi library and BaseModel from pydantic library.\n",
    "\n",
    "Then we configured the uri of the tracker server for MlFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://mlflow.docsystem.xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the model specifying an uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the two labels : \"BENIGN\" and \"DDoS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_NUM = ['BENIGN', 'DDoS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the column names mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_MAPPINGS = {\n",
    "    \"flow_duration\": \"Flow Duration\",\n",
    "    \"fwd_packet_length_std\": \"Fwd Packet Length Std\",\n",
    "    \"bwd_packet_length_mean\": \"Bwd Packet Length Mean\",\n",
    "    \"flow_bytes_s\": \"Flow Bytes/s\",\n",
    "    \"flow_packets_s\": \"Flow Packets/s\",\n",
    "    \"flow_iat_mean\": \"Flow IAT Mean\",\n",
    "    \"flow_iat_std\": \"Flow IAT Std\",\n",
    "    \"bwd_iat_total\": \"Bwd IAT Total\",\n",
    "    \"bwd_iat_mean\": \"Bwd IAT Mean\",\n",
    "    \"bwd_iat_std\": \"Bwd IAT Std\",\n",
    "    \"bwd_iat_max\": \"Bwd IAT Max\",\n",
    "    \"bwd_packets_s\": \"Bwd Packets/s\",\n",
    "    \"active_mean\": \"Active Mean\",\n",
    "    \"active_std\": \"Active Std\",\n",
    "    \"active_max\": \"Active Max\",\n",
    "    \"idle_std\": \"Idle Std\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created an instance of FastAPI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a function whose aim is to transform a dictionary into a dataframe, and another function whose aim to scale the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def to_df(self):\n",
    "        df = pd.DataFrame([self.dict()])\n",
    "        # Change column labels\n",
    "        for column in df.columns:\n",
    "            df = df.rename(columns={column: COLUMN_MAPPINGS[column]})\n",
    "        return df\n",
    "\n",
    "    def scale_df(self):\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.transform(self.to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we made a route FastAPI that returns the predictions of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post('/reports/')\n",
    "async def create_report(report: Report):\n",
    "    # Make predictions\n",
    "    predictions = model.predict(report.scale_df())\n",
    "    print(predictions)\n",
    "    predictions = [LABELS_NUM[prediction] for prediction in predictions.tolist()]\n",
    "    return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MlFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
